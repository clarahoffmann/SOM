{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.Doc2VecModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsvT4DMbc7-k",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Comparing Twitter behavior of German politicians  before and after the European Election 2019 using Self-Organizing Maps\n",
        " \n",
        "**1. Translate Tweets into numerical data\n",
        "     using Doc2Vec**\n",
        "\n",
        "Student Project on Self-Organizing Maps \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Authors: Clara Hoffmann & Oliver Becker\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4tbapUsyKl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set workspace\n",
        "# we worked in google colab so you might\n",
        "# need to change this line to your working\n",
        "# directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VvgO3U-G26Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set path where you saved the data\n",
        "# Clara\n",
        "path = \"/content/drive/My Drive/\"\n",
        "# Oliver\n",
        "#path = \"/content/drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqKElaMwxvqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standard packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "# packages for natural language processing\n",
        "import gensim \n",
        "!pip install nltk\n",
        "import string\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import nltk\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from six import string_types\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWE-unnVeYGu",
        "colab_type": "text"
      },
      "source": [
        "**Data preparation**\n",
        "\n",
        "Subset to 4 weeks before and after the election. Clean from stopwords, usernames, urls and special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrHT0BZU_cLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set column range wide so we can see the more of the tweets\n",
        "pd.get_option('max_colwidth')\n",
        "pd.set_option('max_colwidth', 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6dgXrm6aio7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the data\n",
        "databig =pd.read_csv((path + 'out.csv'), sep=',') #.iloc[:, 1:]\n",
        "# rename first column, which contains date\n",
        "databig=databig.rename(columns = {'Unnamed: 0':'date'})\n",
        "databig # show data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSDq-cSRlYO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert date to string\n",
        "databig['date'] = databig['date'].apply(str)\n",
        "# convert to datetime format for subsetting\n",
        "databig['date'] = databig['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# subset to before election at 6 in the afternoon\n",
        "filter_pre = (databig['date'] < '2019-05-26 18:00:00') & (databig['date'] >= '2019-04-26 18:00:00')\n",
        "data_pre = databig.loc[filter_pre, :]\n",
        "data_pre = data_pre.reset_index(drop=True)\n",
        "data_pre['time'] = 'pre'\n",
        "\n",
        "# subset to one month after the election at 6 in the afternoon\n",
        "filter_post = (databig['date'] < '2019-06-26 18:00:00') & (databig['date'] >= '2019-05-26 18:00:00')\n",
        "data_post = databig.loc[filter_post, :]\n",
        "data_post = data_post.reset_index(drop=True)\n",
        "data_post['time'] = 'post'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTJOokGQM4Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine pre and post election data\n",
        "data = data_pre.append(data_post)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNO3ldr2adfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete special characters from tweets\n",
        "data['tweet'] = data['tweet'].apply(lambda x: re.sub('([\\.\\\",\\(\\)!\\?;:])[!@#$:+).;,?&]1234567890/', '', x.lower()))\n",
        "data['tweet'] = data['tweet'].apply(lambda x: re.sub('  ', ' ', x))\n",
        "data['tweet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2pfxzE_kJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some summary statistics:\n",
        "# 1. words per tweet\n",
        "data['word_count'] = data['tweet'].str.count(' ') + 1\n",
        "data.groupby('party')['word_count'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46znSox1BJzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. words per tweet and politician\n",
        "data.groupby('real')['word_count'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aiXW1UPBtaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3. Posts per Politician\n",
        "data['real'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_enX_iuubR82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# next we delete all stopwords \n",
        "# this is a standard procedure to only\n",
        "# use the words that carry a lot of meaning\n",
        "# in the Doc2Vec model\n",
        "stopwords.words('german')[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GDMPf1rGd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to remove stopwords, @username and urls\n",
        "def cleaning(data):\n",
        "  print(\"start cleaning vector\")\n",
        "  \n",
        "  df = data\n",
        "  # remove stopwords from data\n",
        "  stop = set(stopwords.words('german', 'english')) \n",
        "  df['newtweet'] = df['tweet'].str.split()\n",
        "  df['newtweet'] = df['newtweet'].apply(lambda x : [item for item in x if item not in stop])\n",
        "  df[\"newtweet\"]= df[\"newtweet\"].str.join(\" \") \n",
        "  \n",
        "  # remove @usernames\n",
        "  df['newtweet'] = df['newtweet'].apply(lambda x : re.sub(r'@[A-Za-z0-9]+','',x) )\n",
        "  \n",
        "  # remove urls\n",
        "  df['newtweet'] = df['newtweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "  print(\"finished cleaning vector\")\n",
        "  return(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T50YtA3grzZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean data\n",
        "df = cleaning(data)\n",
        "df['newtweet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiPEFO0PaOPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save data\n",
        "df.to_csv((path + 'data_cleaned.csv'), sep=',', encoding='utf-8', index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz0-fcHqXFGq",
        "colab_type": "text"
      },
      "source": [
        "**Training the Doc2Vec model**\n",
        "\n",
        "\n",
        "\n",
        "Train our Doc2Vec model. The model uses a neural network to depict similarity between sentences (in our case between tweets) as a numerical vector, which is exactly what we need as input for our SOM. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcDBPe6WGF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define function to train Doc2Vec model\n",
        "def trainmodel(cleanedtweets, max_epochs, vec_size, alpha , modelname):\n",
        "  \n",
        "  # tag data to identify semantic structures\n",
        "  tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), \n",
        "                                tags=[str(i)]) for i, _d in \n",
        "                 enumerate(cleanedtweets)]\n",
        "\n",
        "  model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "  model.build_vocab(tagged_data)\n",
        "  \n",
        "  for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "  model.save(str(modelname))\n",
        "  print(\"Model\" + str(modelname) +\"Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTjfY19KYK-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# join tweets over all columns \n",
        "col = []\n",
        "names = pd.unique(data['real'])\n",
        "time = pd.unique(data['time'])\n",
        "for y in time:\n",
        " subset_time = data[data['time'] == y]\n",
        " for x in range(0,len(names)):\n",
        "  subset_name = subset_time[subset_time['real'] == names[x]]\n",
        "  all = subset_name['newtweet'].str.cat(sep=' ')\n",
        "  col.append({'name': names[x], 'tweets' :  all, 'time': y })\n",
        "\n",
        "aggdata  = pd.DataFrame(col)\n",
        "aggdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU2SxmpVX_5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train our model where one observation is pre or post of a politician's assembly of tweet\n",
        "aggmydata = aggdata['tweets']\n",
        "trainmodel(cleanedtweets = aggmydata, max_epochs = 100, vec_size = 200, alpha = 0.0025, modelname = \"aggmodel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CajpOKReyMWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load trained model\n",
        "modelagg = Doc2Vec.load(\"aggmodel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riw06gbyZJSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some summaries to get an impression of what our model looks like\n",
        "# fetch value of vector of first tweet\n",
        "print(modelagg.docvecs['3'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHrD9pMYHa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of words in our vocabulary\n",
        "len(modelagg.wv.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8egTzotSYjJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of trained document tags\n",
        "len(modelagg.docvecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSZoRJ5AHIQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to assemble our model back into a dataframe\n",
        "# that we can use as input for our SOM\n",
        "def assemble(data, model):\n",
        "  # create indexvector to make labeling df easier\n",
        "  # in case we want to change the vector size of the \n",
        "  # Doc2Vec model\n",
        "  indexvec = []\n",
        "  for col in range(0, len(model.docvecs[1])): \n",
        "    indexvec.append(\"x\" + str(col+1))\n",
        "  indexvec\n",
        "\n",
        "\n",
        "   # assemble word vector as dataframe\n",
        "  wordvector = pd.DataFrame(model.docvecs[0], [indexvec])\n",
        "  for col in range(0, len(data)): \n",
        "   wordvector[col] = pd.DataFrame(model.docvecs[col], [indexvec])\n",
        "  wordvector\n",
        "  joint_data = pd.concat([data, wordvector.T ], axis=1, sort=False)\n",
        "  indexvec2 = data.columns.tolist()\n",
        "  for col in range(0, len(indexvec)): \n",
        "     indexvec2.append(indexvec[col])\n",
        "  joint_data.columns = indexvec2\n",
        "  return(joint_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwJGk-GmXr_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assemble data and check whether it looks fine\n",
        "personb = assemble(aggdata, modelagg)\n",
        "personb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK46Ww6-UDFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save dataframe \n",
        "personb.to_csv((path + 'data_wordvectors_pers.csv'), sep=',', encoding='utf-8', index=True)\n",
        "# save data for PCA \n",
        "from google.colab import files\n",
        "personb.to_csv('forPCA.csv') \n",
        "files.download('forPCA.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}