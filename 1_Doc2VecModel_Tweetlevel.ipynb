{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.Doc2VecModel_Tweetlevel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyngoe0qWlD0",
        "colab_type": "text"
      },
      "source": [
        "Load workspace - we need this to be able to download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4tbapUsyKl3",
        "colab_type": "code",
        "outputId": "3adda1fd-2a06-4e5d-8202-533220b09980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqKElaMwxvqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from minisom import MiniSom Jupyter Notebook \n",
        "# standard packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "import sys\n",
        "# packages for natural language processing\n",
        "import gensim \n",
        "!pip install minisom\n",
        "!pip install nltk\n",
        "from minisom import MiniSom\n",
        "import string\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import nltk\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrHT0BZU_cLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.get_option('max_colwidth')\n",
        "pd.set_option('max_colwidth', 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpgtZqR3RvNh",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6dgXrm6aio7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data =pd.read_csv('/content/gdrive/My Drive/out.csv', sep=',') #.iloc[:, 1:]\n",
        "# rename first column, which contains date\n",
        "data=data.rename(columns = {'Unnamed: 0':'date'})\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln-cS370bMYI",
        "colab_type": "text"
      },
      "source": [
        "Delete special characters from tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNO3ldr2adfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['tweet'] = data['tweet'].apply(lambda x: re.sub('([\\.\\\",\\(\\)!\\?;:])[!@#$:+).;,?&]1234567890/', '', x.lower()))\n",
        "data['tweet'] = data['tweet'].apply(lambda x: re.sub('  ', ' ', x))\n",
        "data['tweet'][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNaHNJG1Bjfv",
        "colab_type": "text"
      },
      "source": [
        "Some summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2pfxzE_kJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['word_count'] = data['tweet'].str.count(' ') + 1\n",
        "# words per tweet\n",
        "data.groupby('party')['word_count'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46znSox1BJzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words per tweet and politician\n",
        "data.groupby('real')['word_count'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aiXW1UPBtaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Posts per Politician\n",
        "data['real'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_enX_iuubR82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords.words('german')[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuvR6ncjVKIJ",
        "colab_type": "text"
      },
      "source": [
        "Clean data for Doc2Vec:\n",
        "Remove stop words, links, usernames\n",
        "**To do: remove hashtags (?) and rt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GDMPf1rGd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stopwords\n",
        "def cleaning(data):\n",
        "  print(\"start cleaning vector\")\n",
        "  \n",
        "  df = data\n",
        "  # remove stopwords from data\n",
        "  stop = set(stopwords.words('german', 'english')) \n",
        "  df['newtweet'] = df['tweet'].str.split()\n",
        "  df['newtweet'] = df['newtweet'].apply(lambda x : [item for item in x if item not in stop])\n",
        "  df[\"newtweet\"]= df[\"newtweet\"].str.join(\" \") \n",
        "  \n",
        "  # remove @usernames\n",
        "  df['newtweet'] = df['newtweet'].apply(lambda x : re.sub(r'@[A-Za-z0-9]+','',x) )\n",
        "  \n",
        "  # remove urls\n",
        "  df['newtweet'] = df['newtweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "  print(\"finished cleaning vector\")\n",
        "  return(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T50YtA3grzZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = cleaning(data)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiPEFO0PaOPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save data\n",
        "df.to_csv('/content/gdrive/My Drive/data_cleaned.csv', sep=',', encoding='utf-8', index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHziqEAnWzZ3",
        "colab_type": "text"
      },
      "source": [
        "Tag Data: helps to identify semantic structures in sentences, necessary for our doc2vec training\n",
        "source: https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz0-fcHqXFGq",
        "colab_type": "text"
      },
      "source": [
        "Train our Doc2Vec model.\n",
        "The model uses a neural network to depict similarity between sentences (in our case between tweets) as a numerical vector, which is exactly what we need as input for our SOM!\n",
        "\n",
        "**To Do: research on how to choose parameters**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcDBPe6WGF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainmodel(cleanedtweets, max_epochs, vec_size, alpha , modelname):\n",
        "  \n",
        "  # tag data to identify semantic structures\n",
        "  tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), \n",
        "                                tags=[str(i)]) for i, _d in \n",
        "                 enumerate(data['newtweet'])]\n",
        "\n",
        "  model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "  model.build_vocab(tagged_data)\n",
        "  \n",
        "  for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "  model.save(str(modelname))\n",
        "  print(\"Model\" + str(modelname) +\"Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cffyl_0Ubj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata = data['newtweet']\n",
        "trainmodel(cleanedtweets = mydata, max_epochs = 100, vec_size = 30, alpha = 0.0025, modelname = \"modelbasic\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTjfY19KYK-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# join tweets over all columns \n",
        "col = []\n",
        "names = pd.unique(data['real'])\n",
        "for x in range(0,len(names)):\n",
        " subset = data[data['real'] == names[x]]\n",
        " all = subset['newtweet'].str.cat(sep=' ')\n",
        " col.append({'name': names[x], 'tweets' :  all })\n",
        "\n",
        "aggdata  = pd.DataFrame(col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBdzpg1OYTyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU2SxmpVX_5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggmydata = aggdata['tweets']\n",
        "trainmodel(cleanedtweets = aggmydata, max_epochs = 100, vec_size = 30, alpha = 0.0025, modelname = \"aggmodel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXKCWPO2ZMKc",
        "colab_type": "text"
      },
      "source": [
        "Load our trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CajpOKReyMWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model= Doc2Vec.load(\"modelbasic\")\n",
        "modelagg = Doc2Vec.load(\"aggmodel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riw06gbyZJSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fetch value of vector of first tweet\n",
        "print(model.docvecs['1'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHrD9pMYHa7",
        "colab_type": "code",
        "outputId": "ab6a8bd6-ab4c-449d-dc53-f111b977b967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of words in our vocabulary\n",
        "len(model.wv.vocab)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8egTzotSYjJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of trained document tags\n",
        "len(model.docvecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4f5L2PzKj5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length of word vector\n",
        "len(model.docvecs[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSZoRJ5AHIQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def assemble(data, model):\n",
        "  #create indexvector to make labeling df easier\n",
        "  # in case we want to change the vector size of the \n",
        "  # Doc2Vec model\n",
        "  indexvec = []\n",
        "  for col in range(0, len(model.docvecs[1])): \n",
        "    indexvec.append(\"x\" + str(col+1))\n",
        "  indexvec\n",
        "\n",
        "\n",
        "   # assemble word vector as dataframe\n",
        "  wordvector = pd.DataFrame(model.docvecs[1], [indexvec])\n",
        "  for col in range(1, len(data)): \n",
        "   wordvector[col] = pd.DataFrame(model.docvecs[col], [indexvec])\n",
        "  wordvector\n",
        "  joint_data = pd.concat([data, wordvector.T ], axis=1, sort=False)\n",
        "  return(joint_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q4QMjy8XfpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obsb = assemble(data, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwJGk-GmXr_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personb = assemble(aggdata, modelagg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK46Ww6-UDFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save dfs so we can load them into our SOM code\n",
        "obsb.to_csv('/content/gdrive/My Drive/data_wordvectors_obs.csv', sep=',', encoding='utf-8', index=True)\n",
        "personb.to_csv('/content/gdrive/My Drive/data_wordvectors_pers.csv', sep=',', encoding='utf-8', index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}