{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyngoe0qWlD0",
        "colab_type": "text"
      },
      "source": [
        "Load workspace - we need this to be able to download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4tbapUsyKl3",
        "colab_type": "code",
        "outputId": "388d3824-53d4-4ff4-a5c0-bf970c951a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dKiWfLSWtWr",
        "colab_type": "text"
      },
      "source": [
        "Load Packages - muss ich noch aufräumen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqKElaMwxvqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "5be8193e-f107-4271-ef2c-eb789aae3dff"
      },
      "source": [
        "#from minisom import MiniSom\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "import sys\n",
        "import gzip\n",
        "import gensim \n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "!pip install minisom\n",
        "!pip install nltk\n",
        "from minisom import MiniSom\n",
        "import string\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import nltk\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from random import randint\n",
        "from collections import namedtuple\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        " \n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        " \n",
        " \n",
        "pd.options.mode.chained_assignment = None\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting minisom\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/33/f71756da20137740008d6f99f861e0b5df74b5fa08d032dcb02dfe19c128/MiniSom-2.1.6.tar.gz\n",
            "Building wheels for collected packages: minisom\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/46/49/a920ae8083e5da81a42e90a799fb73a0bc52006d927197e50b\n",
            "Successfully built minisom\n",
            "Installing collected packages: minisom\n",
            "Successfully installed minisom-2.1.6\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrHT0BZU_cLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.get_option('max_colwidth')\n",
        "pd.set_option('max_colwidth', 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6dgXrm6aio7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data =pd.read_csv('/content/gdrive/My Drive/out.csv', sep=',').iloc[:, 1:]\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln-cS370bMYI",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the data: delete special characters, stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNO3ldr2adfZ",
        "colab_type": "code",
        "outputId": "68a69b58-0061-44c7-90cf-2b6f45eb036d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data['tweet'] = data['tweet'].apply(lambda x: re.sub('([\\.\\\",\\(\\)!\\?;:])[!@#$:+).;,?&]1234567890/', '', x.lower()))\n",
        "data['tweet'] = data['tweet'].apply(lambda x: re.sub('  ', ' ', x))\n",
        "data['tweet'][1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rt @bobcdu: in #paris hat @akk auch nochmals jegliche zusammenarbeit mit der rechtspopulistischen afd kategorisch ausgeschlossen. rechtspop…'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNaHNJG1Bjfv",
        "colab_type": "text"
      },
      "source": [
        "Some summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2pfxzE_kJc",
        "colab_type": "code",
        "outputId": "4735096f-312f-4610-bb26-fb6fc6f2b8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "data['word_count'] = data['tweet'].str.count(' ') + 1\n",
        "# words per tweet\n",
        "data.groupby('party')['word_count'].mean()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "party\n",
              "AfD      18.000000\n",
              "CDU      16.567568\n",
              "FDP      14.500000\n",
              "Grüne    18.800000\n",
              "Linke    17.700000\n",
              "SPD      18.675000\n",
              "Name: word_count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46znSox1BJzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words per tweet and politician\n",
        "data.groupby('real')['word_count'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r65xdr4dgAb6",
        "colab_type": "text"
      },
      "source": [
        "work in progress: müssen irgendwie noch die stopwords in englisch und deutsch entfernen aber geht gerade nicht so gut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aiXW1UPBtaU",
        "colab_type": "code",
        "outputId": "ffee09fd-bab9-4fd2-8db8-771a1f58c3a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Posts per Politician\n",
        "data['real'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sarah Wagenknecht              20\n",
              "Jörg Meuthen                   20\n",
              "Cem Özdemir                    20\n",
              "Christian Lindner              20\n",
              "Annegreth Kramp-Karrenbauer    20\n",
              "Katharina Barley               20\n",
              "Kathrin Goering-Eckardt        20\n",
              "Alice Weidel                   20\n",
              "Andrea Nahles                  20\n",
              "Angela Merkel                  17\n",
              "Name: real, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrXFLiNwu8pE",
        "colab_type": "text"
      },
      "source": [
        "Remove stopwords, links, special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_enX_iuubR82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords.words('german')[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GDMPf1rGd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stopwords\n",
        "stop = set(stopwords.words('german', 'english')) \n",
        "data['newtweet'] = data['tweet'].str.split()\n",
        "data['newtweet'] = data['newtweet'].apply(lambda x : [item for item in x if item not in stop])\n",
        "data[\"newtweet\"]= data[\"newtweet\"].str.join(\" \") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T50YtA3grzZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['newtweet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjzmWhZJvgaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove urls\n",
        "data['newtweet'] = data['newtweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHziqEAnWzZ3",
        "colab_type": "text"
      },
      "source": [
        "Tag Data: helps to identify semantic structures in sentences, necessary for our doc2vec training\n",
        "source: https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBy3JjexRRny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data['newtweet'])]\n",
        "tagged_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7zZbILtpxZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "c45799a7-3b36-468d-d175-115409c68b1c"
      },
      "source": [
        "tagged_data = [w for w in tagged_data if not w in stop_words] "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0051627ddef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-0051627ddef5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz0-fcHqXFGq",
        "colab_type": "text"
      },
      "source": [
        "Train our Doc2Vec model -> i think we need to readjust the parameters\n",
        "The model uses a neural network to depict similarity between sentences (in our case between tweets) as a numerical vector, which is exactly what we need as input for our SOM!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcDBPe6WGF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epochs = 100\n",
        "vec_size = 20\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXKCWPO2ZMKc",
        "colab_type": "text"
      },
      "source": [
        "Load our trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CajpOKReyMWG",
        "colab_type": "code",
        "outputId": "72c4cf6d-22f5-4147-8824-d16eb4badfb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "model= Doc2Vec.load(\"d2v.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-21 12:45:57,673 : INFO : loading Doc2Vec object from d2v.model\n",
            "2019-06-21 12:45:57,691 : INFO : loading vocabulary recursively from d2v.model.vocabulary.* with mmap=None\n",
            "2019-06-21 12:45:57,693 : INFO : loading trainables recursively from d2v.model.trainables.* with mmap=None\n",
            "2019-06-21 12:45:57,694 : INFO : loading wv recursively from d2v.model.wv.* with mmap=None\n",
            "2019-06-21 12:45:57,699 : INFO : loading docvecs recursively from d2v.model.docvecs.* with mmap=None\n",
            "2019-06-21 12:45:57,701 : INFO : loaded d2v.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riw06gbyZJSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fetch value of vector of first tweet\n",
        "print(model.docvecs['1'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHrD9pMYHa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of words in our vocabulary\n",
        "len(model.wv.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8egTzotSYjJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of trained document tags\n",
        "len(model.docvecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4f5L2PzKj5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length of word vector\n",
        "len(model.docvecs[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBt_LbFLQue6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordvector = pd.DataFrame(model.docvecs[1], [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\",\n",
        "                  \"x6\", \"x7\", \"x8\", \"x9\", \"x10\",\n",
        "                  \"x11\", \"x12\", \"x13\", \"x14\", \"x15\",\n",
        "                  \"x16\", \"x17\", \"x18\", \"x19\", \"x20\"])\n",
        "wordvector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzJTgBlIR-eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in range(1, len(data)): \n",
        " wordvector[col] = pd.DataFrame(model.docvecs[col], [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\",\n",
        "                  \"x6\", \"x7\", \"x8\", \"x9\", \"x10\",\n",
        "                  \"x11\", \"x12\", \"x13\", \"x14\", \"x15\",\n",
        "                  \"x16\", \"x17\", \"x18\", \"x19\", \"x20\"])\n",
        "wordvector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9p02Fq9TVsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joint_data = pd.concat([data, wordvector.T ], axis=1, sort=False)\n",
        "joint_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK46Ww6-UDFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joint_data.to_csv('data_wordvectors.csv', sep=',', encoding='utf-8', index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Z34dKegQzE",
        "colab_type": "text"
      },
      "source": [
        "**TRASH CODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLPVDvodr-Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword_set = set(stopwords.words('german'))\n",
        "tokenizer = TweetTokenizer\n",
        "MessageDoc = namedtuple('MessageDoc', 'words tags')\n",
        "\n",
        "alldocs = []  # Will hold all doacs in original order\n",
        "for line_no, line in data.iterrows():\n",
        "    message = line.message.lower()\n",
        "    message = tokenizer().tokenize(message) \n",
        "    #import pdb; pdb.set_trace()\n",
        "    words = [word.strip('#') for word in message if (word not in stopword_set \n",
        "                                          and word not in string.punctuation\n",
        "                                          and not word.startswith('http')\n",
        "                                          and not re.search(r'\\d', word))]\n",
        "    tags = [str(line_no), line['from_name']] #, line['Partei_ABK']] # line_no needs to be converted as string to be included in tags \n",
        "    alldocs.append(MessageDoc(words, tags))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZfFRuJVvp4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "model = gensim.models.Doc2Vec(dm=0, dbow_words=0, size=100, negative=5, hs=0, min_count=5, workers=cores),\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTwDzLh-vr2V",
        "colab_type": "code",
        "outputId": "1615ff79-dc7f-489f-a48e-2904652798cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "for name, train_model in models_by_name.items():\n",
        "    train_model.train(alldocs, total_examples=len(alldocs), epochs=epochs, start_alpha=0.025, end_alpha=0.001)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-12a6aad0011d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_by_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models_by_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mf10MlYs63n",
        "colab_type": "code",
        "outputId": "31a3829e-5a4a-47b1-bd52-a9a82ba3eb22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UIvaloxsF52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = nlp_clean(tweets['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y67Cs13tM5e",
        "colab_type": "code",
        "outputId": "10597190-06f0-4011-f67b-7140891faac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Choose a random document/candidate\n",
        "i = randint(0, 5-1)\n",
        "i\n",
        "tweets.iloc[i,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name                                     Katharina Barley\n",
              "text    ['Herzlichen Glückwunsch liebe Christine #Lamb...\n",
              "Name: 2, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vWxoIxCxvr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean tweets from special characters\n",
        "tweets['text'] = tweets['text'].apply(lambda x: re.sub('([\\.\\\",\\(\\)!\\?;:])[!@#$:+).;,?&]1234567890/', '', x.lower()))\n",
        "tweets['text'] = tweets['text'].apply(lambda x: re.sub('  ', ' ', x))\n",
        "tweets['text'][1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47SWnLyqxvsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# produce corpus with words in format for gensim Word2vec\n",
        "tmp_corpus = tweets['text'].map(lambda x: x.split('.'))\n",
        "tmp_corpus \n",
        "corpus = []\n",
        "for i in tqdm(range(len(tmp_corpus))):\n",
        "    for line in tmp_corpus[i]:\n",
        "        words = [x for x in line.split()]\n",
        "        corpus.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckk-eJ8SxvsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_sentences = len(corpus)\n",
        "num_of_words = 0\n",
        "for line in corpus:\n",
        "    num_of_words += len(line)\n",
        "\n",
        "print('Num of sentences - %s'%(num_of_sentences))\n",
        "print('Num of words - %s'%(num_of_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2gMAwnlZxvsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = gensim.models.Doc2Vec(\n",
        "        #corpus,\n",
        "        #size=150,\n",
        "        #window=10,\n",
        "        #min_count=2,\n",
        "        #workers=5,\n",
        "       # iter=10)\n",
        " model = Doc2Vec(corpus, vector_size=5, window=2, min_count=1, workers=4)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7EteG2iqDGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMlR58Q1pvkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA6YhXHaxvs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wl = \"heute\"\n",
        "model.wv.most_similar (positive = wl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhJKstgKxvtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(model.wv.vocab)\n",
        "model.wv.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRCMBWiuxvta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "som = MiniSom(6, 6, 4, sigma=0.3, learning_rate=0.5)\n",
        "som.pca_weights_init(tweets)\n",
        "print(\"Training...\")\n",
        "som.train_random(tweets, 100, verbose=True)\n",
        "print(\"\\n...ready!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tURuIQZ6xvtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv['heute']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgPUzhd0jkrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFut2KA5xvt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_dim = 16\n",
        "som = MiniSom(map_dim, map_dim, 50, sigma=1.0, random_seed=1)\n",
        "som.train_batch(W,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44opJrBZlloB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# load the digits dataset from scikit-learn\n",
        "digits = datasets.load_digits(n_class=10)\n",
        "digits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLCugvEDmwSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = scale(data)\n",
        "num = digits.target  # num[i] is the digit represented by data[i]\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emSw6XUOBdJU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}